{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SUMM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBbVPex0vwJd",
        "colab_type": "code",
        "outputId": "ca37ebdf-4131-4124-9390-32a149680cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution() \n",
        "import csv\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import json, gzip\n",
        "from pandas.io.json import json_normalize\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnXYZ0gAvz1n",
        "colab_type": "code",
        "outputId": "ab560c41-401b-44e7-9fe3-f7597c89c2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvC6aC2Xvz8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    printable = set(string.printable)\n",
        "    text = \"\".join(list(filter(lambda x: x in printable, text))) #filter funny characters, if any.\n",
        "    return text\n",
        "\n",
        "text_max_len = 2000\n",
        "text_min_len = 250\n",
        "summary_max_len = 40\n",
        "vocab2idx = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de3148Asvz_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_train = \"/content/drive/My Drive/Colab Notebooks/summ/release/train.jsonl.gz\"\n",
        "#path_val = \"/content/drive/My Drive/Colab Notebooks/summ/release/dev.jsonl.gz\"\n",
        "#path_test = \"/content/drive/My Drive/Colab Notebooks/summ/release/test.jsonl.gz\"\n",
        "data_train = []\n",
        "data_val = []\n",
        "data_test = []\n",
        "\n",
        "with gzip.open(path_train) as f:\n",
        "    for ln in f:\n",
        "        obj = json.loads(ln)\n",
        "        data_train.append(obj)\n",
        "'''\n",
        "with gzip.open(path_val) as f:\n",
        "    for ln in f:\n",
        "        obj = json.loads(ln)\n",
        "        data_val.append(obj)\n",
        "\n",
        "with gzip.open(path_test) as f:\n",
        "    for ln in f:\n",
        "        obj = json.loads(ln)\n",
        "        data_test.append(obj)\n",
        "df_val = pd.DataFrame.from_dict(json_normalize(data_val), orient='columns')\n",
        "df_test = pd.DataFrame.from_dict(json_normalize(data_test), orient='columns')\n",
        "'''\n",
        "df_train = pd.DataFrame.from_dict(json_normalize(data_train), orient='columns')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZATXXKwMv0Bh",
        "colab_type": "code",
        "outputId": "4cf4711c-5520-4da7-def8-81f7c445debf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_s_train = df_train[['text', 'summary']]\n",
        "data_s_train.head()\n",
        "Text = data_s_train['text']\n",
        "Summary = data_s_train['summary']\n",
        "print(len(Summary))\n",
        "print(len(Text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "995041\n",
            "995041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwrhrw--v0EL",
        "colab_type": "code",
        "outputId": "a88b259c-9033-46e7-c58e-af74aefc40b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "txt = []\n",
        "summ = []\n",
        "texts = []\n",
        "summaries = []\n",
        "for i in range(len(Text)):\n",
        "  txt = Text[i]\n",
        "  summ = Summary[i]\n",
        "  if len(txt) <= text_max_len and len(txt) >= text_min_len and len(summ) <= summary_max_len:\n",
        "    clean_text = clean(txt)\n",
        "    clean_summary = clean(summ)\n",
        "    tokenized_summary = word_tokenize(clean_summary)\n",
        "    tokenized_text = word_tokenize(clean_text)\n",
        "\n",
        "    for word in tokenized_text:\n",
        "      if word not in vocab2idx:\n",
        "        vocab2idx[word]=len(vocab2idx)\n",
        "    for word in tokenized_summary:\n",
        "      if word not in vocab2idx:\n",
        "       vocab2idx[word]=len(vocab2idx)\n",
        "  \n",
        "    summaries.append(tokenized_summary)\n",
        "    texts.append(tokenized_text)\n",
        "\n",
        "    if i%10000==0:\n",
        "      print(\"Processing data # {}\".format(i))\n",
        "    i+=1\n",
        "                          \n",
        "print(\"\\n# of Data: {}\".format(len(texts)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# of Data: 15828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaQdz8XWv0Gq",
        "colab_type": "code",
        "outputId": "78030819-0f3b-433a-ec3d-3c8e92d96967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import numpy, random\n",
        "\n",
        "index = random.randint(0, len(texts)-1)\n",
        "print(\"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n{}\\n\\n\".format(texts[index]))\n",
        "print(\"SAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n{}\\n\".format(summaries[index]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAMPLE CLEANED & TOKENIZED TEXT: \n",
            "\n",
            "['the', 'brexit', 'is', 'official', ',', 'with', '52', 'percent', 'of', 'the', 'united', 'kingdom', 'voting', 'to', 'leave', 'the', 'european', 'union', '.', 'even', 'though', 'a', 'lot', 'of', 'people', 'are', 'not', 'happy', 'about', 'the', 'decision', ',', 'and', 'its', 'strong', 'implications', 'on', 'the', 'economy', ',', 'travel', ',', 'education', ',', 'healthcare', 'and', 'culture', ',', 'many', 'wasted', 'no', 'time', 'in', 'taking', 'lemons', 'and', 'making', 'lemonade', '.', 'here', 'are', 'some', 'of', 'the', 'best', 'reactions', ',', 'brought', 'to', 'you', 'by', 'horrified', 'people', '.', 'a', 'lot', 'of', 'them', 'involve', 'crying', 'jordan', '.', 'meanwhile', 'india', 'is', 'just', 'blown', 'away', 'that', 'you', 'can', 'get', 'britain', 'to', 'leave', 'by', 'voting', 'bobby', 'big', 'wheel', '(', '@', 'bobbybigwheel', ')', 'june', '23', ',', '2016', 'have', 'something', 'to', 'add', 'to', 'this', 'story', '?', 'share', 'it', 'in', 'the', 'comments', '.']\n",
            "\n",
            "\n",
            "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
            "\n",
            "['most', 'of', 'them', 'are', '#', 'cryingjordan', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U7TiXGhv0KO",
        "colab_type": "code",
        "outputId": "c12eac24-7404-494d-86f9-a8127e906016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab = []\n",
        "embd = []\n",
        "special_tags = ['<UNK>','<PAD>','<EOS>']\n",
        "\n",
        "\n",
        "def loadEmbeddings(filename):\n",
        "    vocab2embd = {}\n",
        "    \n",
        "    with open(filename) as infile:     \n",
        "        for line in infile:\n",
        "            row = line.strip().split(' ')\n",
        "            word = row[0].lower()\n",
        "            if word not in vocab2embd:\n",
        "                vocab2embd[word]=np.asarray(row[1:],np.float32)\n",
        "\n",
        "    print('Embedding Loaded.')\n",
        "    return vocab2embd\n",
        "\n",
        "vocab2embd = loadEmbeddings('/content/drive/My Drive/Colab Notebooks/Data/glove/glove.6B.100d.txt')\n",
        "\n",
        "for word in vocab2idx:\n",
        "    if word in vocab2embd:\n",
        "        vocab.append(word)\n",
        "        embd.append(vocab2embd[word])\n",
        "        \n",
        "for special_tag in special_tags:\n",
        "    vocab.append(special_tag)\n",
        "    embd.append(np.random.rand(len(embd[0]),))\n",
        "    \n",
        "vocab2idx = {word:idx for idx,word in enumerate(vocab)}\n",
        "embd = np.asarray(embd,np.float32)\n",
        "\n",
        "print(\"Vocabulary Size: {}\".format(len(vocab2idx)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding Loaded.\n",
            "Vocabulary Size: 70150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qevWp8d5v0Lv",
        "colab_type": "code",
        "outputId": "7690dd2f-62b2-4f58-abd2-2f41ca89f458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vocab2idx['<EOS>'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMuHD5crv0Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_texts=[]\n",
        "vec_summaries=[]\n",
        "\n",
        "for text,summary in zip(texts,summaries):\n",
        "    # Replace out of vocab words with index for '<UNK>' tag\n",
        "    vec_texts.append([vocab2idx.get(word,vocab2idx['<UNK>']) for word in text])\n",
        "    vec_summaries.append([vocab2idx.get(word,vocab2idx['<UNK>']) for word in summary])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utaths9zv0Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(101)\n",
        "\n",
        "texts_idx = [idx for idx in range(len(vec_texts))]\n",
        "random.shuffle(texts_idx)\n",
        "\n",
        "vec_texts = [vec_texts[idx] for idx in texts_idx]\n",
        "vec_summaries = [vec_summaries[idx] for idx in texts_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEZpf2GAv0Rg",
        "colab_type": "code",
        "outputId": "3f699ea1-5020-40ae-f5f4-663b53c0ac2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(vec_texts))\n",
        "print(len(vec_summaries))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15828\n",
            "15828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDUSjVyyv0Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use first 10000 data for testing, the next 10000 data for validation, and rest for training\n",
        "\n",
        "test_summaries = vec_summaries[0:10000]\n",
        "test_texts = vec_texts[0:10000]\n",
        "\n",
        "val_summaries = vec_summaries[10000:20000]\n",
        "val_texts = vec_texts[10000:20000]\n",
        "\n",
        "train_summaries = vec_summaries[20000:]\n",
        "train_texts = vec_texts[20000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WmyvMhjv0Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bucket_and_batch(texts,summaries,batch_size=32):\n",
        "    \n",
        "    # Sort summaries and texts according to the length of text\n",
        "    # (So that texts with similar lengths tend to remain in the same batch and thus require less padding)\n",
        "    \n",
        "    text_lens = [len(text) for text in texts]\n",
        "    sortedidx = np.flip(np.argsort(text_lens),axis=0)\n",
        "    texts=[texts[idx] for idx in sortedidx]\n",
        "    summaries=[summaries[idx] for idx in sortedidx]\n",
        "    \n",
        "    batches_text=[]\n",
        "    batches_summary=[]\n",
        "    batches_true_text_len = []\n",
        "    batches_true_summary_len = []\n",
        "    \n",
        "    i=0\n",
        "    while i < (len(texts)-batch_size):\n",
        "        \n",
        "        max_len = len(texts[i])\n",
        "        \n",
        "        batch_text=[]\n",
        "        batch_summary=[]\n",
        "        batch_true_text_len=[]\n",
        "        batch_true_summary_len=[]\n",
        "        \n",
        "        for j in range(batch_size):\n",
        "            \n",
        "            padded_text = texts[i+j]\n",
        "            padded_summary = summaries[i+j]\n",
        "            \n",
        "            batch_true_text_len.append(len(texts[i+j]))\n",
        "            batch_true_summary_len.append(len(summaries[i+j])+1)\n",
        "     \n",
        "            while len(padded_text) < max_len:\n",
        "                padded_text.append(vocab2idx['<PAD>'])\n",
        "\n",
        "            padded_summary.append(vocab2idx['<EOS>']) #End of Sentence Marker\n",
        "            while len(padded_summary) < summary_max_len+1:\n",
        "                padded_summary.append(vocab2idx['<PAD>'])\n",
        "            \n",
        "        \n",
        "            batch_text.append(padded_text)\n",
        "            batch_summary.append(padded_summary)\n",
        "        \n",
        "        batches_text.append(batch_text)\n",
        "        batches_summary.append(batch_summary)\n",
        "        batches_true_text_len.append(batch_true_text_len)\n",
        "        batches_true_summary_len.append(batch_true_summary_len)\n",
        "        \n",
        "        i+=batch_size\n",
        "        \n",
        "    return batches_text, batches_summary, batches_true_text_len, batches_true_summary_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pUZ6g7kv0X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batches_text, train_batches_summary, train_batches_true_text_len, train_batches_true_summary_len \\\n",
        "= bucket_and_batch(train_texts, train_summaries)\n",
        "\n",
        "val_batches_text, val_batches_summary, val_batches_true_text_len, val_batches_true_summary_len \\\n",
        "= bucket_and_batch(val_texts, val_summaries)\n",
        "\n",
        "test_batches_text, test_batches_summary, test_batches_true_text_len, test_batches_true_summary_len \\\n",
        "= bucket_and_batch(test_texts, test_summaries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1RKt7DpwSGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "d = {}\n",
        "\n",
        "d[\"vocab\"] = vocab2idx\n",
        "d[\"embd\"] = embd.tolist()\n",
        "d[\"train_batches_text\"] = train_batches_text\n",
        "d[\"test_batches_text\"] = test_batches_text\n",
        "d[\"val_batches_text\"] = val_batches_text\n",
        "d[\"train_batches_summary\"] = train_batches_summary\n",
        "d[\"test_batches_summary\"] = test_batches_summary\n",
        "d[\"val_batches_summary\"] = val_batches_summary\n",
        "d[\"train_batches_true_text_len\"] = train_batches_true_text_len\n",
        "d[\"val_batches_true_text_len\"] = val_batches_true_text_len\n",
        "d[\"test_batches_true_text_len\"] = test_batches_true_text_len\n",
        "d[\"train_batches_true_summary_len\"] = train_batches_true_summary_len\n",
        "d[\"val_batches_true_summary_len\"] = val_batches_true_summary_len\n",
        "d[\"test_batches_true_summary_len\"] = test_batches_true_summary_len\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/summ/release/processed.json', 'w') as outfile: \n",
        "    json.dump(d, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}