{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SUMM_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niko55/Abstractive_Text_Summarization/blob/master/Bi_LSTM_Abstractive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlHTnPzIvFBr",
        "colab_type": "code",
        "outputId": "ae676ac4-1312-496f-e507-98b6abc3f44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import csv\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import json, gzip\n",
        "from pandas.io.json import json_normalize\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVMypGfqvJJx",
        "colab_type": "code",
        "outputId": "c93a6ee5-1fd3-4405-c93e-b6139c58f6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Pb3XgyvJLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/summ/release/processed.json') as file:\n",
        "\n",
        "    for json_data in file:\n",
        "        saved_data = json.loads(json_data)\n",
        "\n",
        "        vocab2idx = saved_data[\"vocab\"]\n",
        "        embd = saved_data[\"embd\"]\n",
        "        train_batches_text = saved_data[\"train_batches_text\"]\n",
        "        test_batches_text = saved_data[\"test_batches_text\"]\n",
        "        val_batches_text = saved_data[\"val_batches_text\"]\n",
        "        train_batches_summary = saved_data[\"train_batches_summary\"]\n",
        "        test_batches_summary = saved_data[\"test_batches_summary\"]\n",
        "        val_batches_summary = saved_data[\"val_batches_summary\"]\n",
        "        train_batches_true_text_len = saved_data[\"train_batches_true_text_len\"]\n",
        "        val_batches_true_text_len = saved_data[\"val_batches_true_text_len\"]\n",
        "        test_batches_true_text_len = saved_data[\"test_batches_true_text_len\"]\n",
        "        train_batches_true_summary_len = saved_data[\"train_batches_true_summary_len\"]\n",
        "        val_batches_true_summary_len = saved_data[\"val_batches_true_summary_len\"]\n",
        "        test_batches_true_summary_len = saved_data[\"test_batches_true_summary_len\"]\n",
        "\n",
        "        break\n",
        "        \n",
        "idx2vocab = {v:k for k,v in vocab2idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9bhic73vJOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HyperParameters\n",
        "hidden_size = 300\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "max_summary_len = 41 # should be summary_max_len as used in data_preprocessing with +1 (+1 for <EOS>) \n",
        "D = 5 # D determines local attention window size\n",
        "window_len = 2*D+1\n",
        "l2=1e-6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9vTJbuZvJQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "tf.reset_default_graph()\n",
        "\n",
        "embd_dim = len(embd[0])\n",
        "\n",
        "tf_text = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
        "tf_embd = tf.compat.v1.placeholder(tf.float32, [len(vocab2idx), embd_dim])\n",
        "tf_true_summary_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
        "tf_summary = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
        "tf_train = tf.compat.v1.placeholder(tf.bool)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1rNc_xlvJTt",
        "colab_type": "code",
        "outputId": "5d56f94c-14bc-4caa-be66-cb2d13c75344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#Embed vectorize text\n",
        "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text)\n",
        "embd_text = tf.layers.dropout(embd_text,rate=0.3,training=tf_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-4e134df0cdbc>:2: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm4fDBZNvJWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM function\n",
        "def LSTM(x,hidden_state,cell,input_dim,hidden_size,scope):\n",
        "    \n",
        "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
        "        \n",
        "        w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size],\n",
        "                                    dtype=tf.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "        u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size],\n",
        "                            dtype=tf.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "        b = tf.get_variable(\"bias\", shape=[4,1,hidden_size],\n",
        "                    dtype=tf.float32,\n",
        "                    trainable=True,\n",
        "                    initializer=tf.zeros_initializer())\n",
        "        \n",
        "    input_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0])\n",
        "    forget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1])\n",
        "    output_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2])\n",
        "    cell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3])\n",
        "    cell = forget_gate*cell + input_gate*cell_\n",
        "    hidden_state = output_gate*tf.tanh(cell)\n",
        "    \n",
        "    return hidden_state, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd1BZGvAvJYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FORWARD ENCODING\n",
        "S = tf.shape(embd_text)[1] #text sequence length\n",
        "N = tf.shape(embd_text)[0] #batch_size\n",
        "\n",
        "i=0\n",
        "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "hidden_forward=tf.TensorArray(size=S, dtype=tf.float32)\n",
        "\n",
        "#shape of embd_text: [N,S,embd_dim]\n",
        "embd_text_t = tf.transpose(embd_text,[1,0,2]) \n",
        "#current shape of embd_text: [S,N,embd_dim]\n",
        "\n",
        "def cond(i, hidden, cell, hidden_forward):\n",
        "    return i < S\n",
        "\n",
        "def body(i, hidden, cell, hidden_forward):\n",
        "    x = embd_text_t[i]\n",
        "    \n",
        "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"forward_encoder\")\n",
        "    hidden_forward = hidden_forward.write(i, hidden)\n",
        "\n",
        "    return i+1, hidden, cell, hidden_forward\n",
        "\n",
        "_, _, _, hidden_forward = tf.while_loop(cond, body, [i, hidden, cell, hidden_forward])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2HUQpXUvWuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Backward Encoding\n",
        "i=S-1\n",
        "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "hidden_backward=tf.TensorArray(size=S, dtype=tf.float32)\n",
        "\n",
        "def cond(i, hidden, cell, hidden_backward):\n",
        "    return i >= 0\n",
        "\n",
        "def body(i, hidden, cell, hidden_backward):\n",
        "    x = embd_text_t[i]\n",
        "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"backward_encoder\")\n",
        "    hidden_backward = hidden_backward.write(i, hidden)\n",
        "\n",
        "    return i-1, hidden, cell, hidden_backward\n",
        "\n",
        "_, _, _, hidden_backward = tf.while_loop(cond, body, [i, hidden, cell, hidden_backward])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWBuoxzjvWxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Merge Forward and Backward Encoder Hidden States\n",
        "hidden_forward = hidden_forward.stack()\n",
        "hidden_backward = hidden_backward.stack()\n",
        "\n",
        "encoder_states = tf.concat([hidden_forward,hidden_backward],axis=-1)\n",
        "encoder_states = tf.transpose(encoder_states,[1,0,2])\n",
        "\n",
        "encoder_states = tf.layers.dropout(encoder_states,rate=0.3,training=tf_train)\n",
        "\n",
        "final_encoded_state = tf.layers.dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LPLv15GvWzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Implementation of Scoring function\n",
        "def attention_score(encoder_states,decoder_hidden_state,scope=\"attention_score\"):\n",
        "    \n",
        "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
        "        Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size],\n",
        "                                    dtype=tf.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "    encoder_states = tf.reshape(encoder_states,[N*S,2*hidden_size])\n",
        "    \n",
        "    encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size])\n",
        "    decoder_hidden_state = tf.reshape(decoder_hidden_state,[N,2*hidden_size,1])\n",
        "    \n",
        "    return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IRIKGBNvJbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Local Attention Function\n",
        "def align(encoder_states, decoder_hidden_state,scope=\"attention\"):\n",
        "    \n",
        "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
        "        Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,125],\n",
        "                                    dtype=tf.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "        Vp = tf.get_variable(\"Vp\", shape=[125,1],\n",
        "                            dtype=tf.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=tf.glorot_uniform_initializer())\n",
        "    \n",
        "    positions = tf.cast(S-window_len,dtype=tf.float32) # Maximum valid attention window starting position\n",
        "    \n",
        "    # Predict attention window starting position \n",
        "    ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp))\n",
        "    # ps = (soft-)predicted starting position of attention window\n",
        "    pt = ps+D # pt = center of attention window where the whole window length is 2*D+1\n",
        "    pt = tf.reshape(pt,[N])\n",
        "    \n",
        "    i = 0\n",
        "    gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32)\n",
        "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
        "    \n",
        "    def cond(i,gaussian_position_based_scores):\n",
        "        \n",
        "        return i < S\n",
        "                      \n",
        "    def body(i,gaussian_position_based_scores):\n",
        "        \n",
        "        score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma)))) \n",
        "        # (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\n",
        "        gaussian_position_based_scores = gaussian_position_based_scores.write(i,score)\n",
        "            \n",
        "        return i+1,gaussian_position_based_scores\n",
        "                      \n",
        "    i,gaussian_position_based_scores = tf.while_loop(cond,body,[i,gaussian_position_based_scores])\n",
        "    \n",
        "    gaussian_position_based_scores = gaussian_position_based_scores.stack()\n",
        "    gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0])\n",
        "    gaussian_position_based_scores = tf.reshape(gaussian_position_based_scores,[N,S])\n",
        "    \n",
        "    scores = attention_score(encoder_states,decoder_hidden_state)*gaussian_position_based_scores\n",
        "    scores = tf.nn.softmax(scores,axis=-1)\n",
        "    \n",
        "    return tf.reshape(scores,[N,S,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFCknxkgvgTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim],\n",
        "                                dtype=tf.float32,\n",
        "                                trainable=True,\n",
        "                                initializer=tf.glorot_uniform_initializer())\n",
        "    \n",
        "    # SOS represents starting marker \n",
        "    # It tells the decoder that it is about to decode the first word of the output\n",
        "    # I have set SOS as a trainable parameter\n",
        "    \n",
        "    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim],\n",
        "                            dtype=tf.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=tf.glorot_uniform_initializer())\n",
        "    \n",
        "\n",
        "\n",
        "SOS = tf.tile(SOS,[N,1]) #now SOS shape: [N,embd_dim]\n",
        "inp = SOS\n",
        "hidden=final_encoded_state\n",
        "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32)\n",
        "decoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32)\n",
        "outputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32)\n",
        "\n",
        "for i in range(max_summary_len):\n",
        "    \n",
        "    inp = tf.layers.dropout(inp,rate=0.3,training=tf_train)\n",
        "    \n",
        "    attention_scores = align(encoder_states,hidden)\n",
        "    encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
        "    \n",
        "    hidden,cell = LSTM(inp,hidden,cell,embd_dim,2*hidden_size,scope=\"decoder\")\n",
        "    \n",
        "    hidden_ = tf.layers.dropout(hidden,rate=0.3,training=tf_train)\n",
        "    \n",
        "    concated = tf.concat([hidden_,encoder_context_vector],axis=-1)\n",
        "    \n",
        "    linear_out = tf.nn.tanh(tf.matmul(concated,Wc))\n",
        "    decoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0])) \n",
        "    # produce unnormalized probability distribution over vocabulary\n",
        "    \n",
        "    \n",
        "    decoder_outputs = decoder_outputs.write(i,decoder_output)\n",
        "    \n",
        "    # Pick out most probable vocab indices based on the unnormalized probability distribution\n",
        "    \n",
        "    next_word_vec = tf.cast(tf.argmax(decoder_output,1),tf.int32)\n",
        "\n",
        "    next_word_vec = tf.reshape(next_word_vec, [N])\n",
        "\n",
        "    outputs = outputs.write(i,next_word_vec)\n",
        "\n",
        "    next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec)\n",
        "    inp = tf.reshape(next_word, [N, embd_dim])\n",
        "    \n",
        "    \n",
        "decoder_outputs = decoder_outputs.stack()\n",
        "outputs = outputs.stack()\n",
        "\n",
        "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2])\n",
        "outputs = tf.transpose(outputs,[1,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qhSpvauvgVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_trainables = [var for var in tf.trainable_variables() if\n",
        "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
        "                           or \"noreg\" in var.name)]\n",
        "\n",
        "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var\n",
        "                                in filtered_trainables])\n",
        "\n",
        "with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    epsilon = tf.constant(1e-9, tf.float32)\n",
        "\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=tf_summary, logits=decoder_outputs)\n",
        "\n",
        "    pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
        "                                maxlen=max_summary_len,\n",
        "                                dtype=tf.float32)\n",
        "\n",
        "    masked_cross_entropy = cross_entropy*pad_mask\n",
        "\n",
        "    cost = tf.reduce_mean(masked_cross_entropy) + \\\n",
        "        l2*regularization\n",
        "\n",
        "    cross_entropy = tf.reduce_mean(masked_cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y4-RpCcvgwW",
        "colab_type": "code",
        "outputId": "10f79336-49ba-47fc-87f2-84929902472f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Comparing predicted sequence with labels\n",
        "comparison = tf.cast(tf.equal(outputs, tf_summary),\n",
        "                     tf.float32)\n",
        "\n",
        "# Masking to ignore the effect of pads while calculating accuracy\n",
        "pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
        "                            maxlen=max_summary_len,\n",
        "                            dtype=tf.bool)\n",
        "\n",
        "masked_comparison = tf.boolean_mask(comparison, pad_mask)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = tf.reduce_mean(masked_comparison)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6zCnDbKvnM8",
        "colab_type": "code",
        "outputId": "8d2952c3-7f73-4cdf-c669-0b1035ddd41a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "optimizer = tf.contrib.opt.NadamOptimizer(\n",
        "    learning_rate=learning_rate)\n",
        "\n",
        "gvs = optimizer.compute_gradients(cost, var_list=all_vars)\n",
        "\n",
        "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] # Gradient Clipping\n",
        "\n",
        "train_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L55jzgAXvnPi",
        "colab_type": "code",
        "outputId": "77ee0f1a-fb44-4dd8-a3db-908a0f1d486c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "with tf.Session() as sess:  # Start Tensorflow Session\n",
        "    display_step = 100\n",
        "    patience = 5\n",
        "\n",
        "    load = input(\"\\nLoad checkpoint? y/n: \")\n",
        "    print(\"\")\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    if load.lower() == 'y':\n",
        "\n",
        "        print('Loading pre-trained weights for the model...')\n",
        "        saver.restore(sess, '/content/drive/My Drive/Colab Notebooks/summ/release/Model_Backup/Seq2seq_summarization.ckpt')\n",
        "        sess.run(tf.global_variables())\n",
        "        sess.run(tf.tables_initializer())\n",
        "\n",
        "        with open('/content/drive/My Drive/Colab Notebooks/summ/release/Model_Backup/Seq2seq_summarization.pkl', 'rb') as fp:\n",
        "            train_data = pickle.load(fp)\n",
        "\n",
        "        covered_epochs = train_data['covered_epochs']\n",
        "        best_loss = train_data['best_loss']\n",
        "        impatience = 0\n",
        "        \n",
        "        print('\\nRESTORATION COMPLETE\\n')\n",
        "\n",
        "    else:\n",
        "        best_loss = 2**30\n",
        "        impatience = 0\n",
        "        covered_epochs = 0\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        sess.run(tf.tables_initializer())\n",
        "\n",
        "    epoch=0\n",
        "    while (epoch+covered_epochs)<epochs:\n",
        "        \n",
        "        print(\"\\n\\nSTARTING TRAINING\\n\\n\")\n",
        "        \n",
        "        batches_indices = [i for i in range(0, len(train_batches_text))]\n",
        "        random.shuffle(batches_indices)\n",
        "\n",
        "        total_train_acc = 0\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for i in range(0, len(train_batches_text)):\n",
        "            \n",
        "            j = int(batches_indices[i])\n",
        "\n",
        "            cost,prediction,\\\n",
        "                acc, _ = sess.run([cross_entropy,\n",
        "                                   outputs,\n",
        "                                   accuracy,\n",
        "                                   train_op],\n",
        "                                  feed_dict={tf_text: train_batches_text[j],\n",
        "                                             tf_embd: embd,\n",
        "                                             tf_summary: train_batches_summary[j],\n",
        "                                             tf_true_summary_len: train_batches_true_summary_len[j],\n",
        "                                             tf_train: True})\n",
        "            \n",
        "            total_train_acc += acc\n",
        "            total_train_loss += cost\n",
        "\n",
        "            if i % display_step == 0:\n",
        "                print(\"Iter \"+str(i)+\", Cost= \" +\n",
        "                      \"{:.3f}\".format(cost)+\", Acc = \" +\n",
        "                      \"{:.2f}%\".format(acc*100))\n",
        "            \n",
        "            if i % 500 == 0:\n",
        "                \n",
        "                idx = random.randint(0,len(train_batches_text[j])-1)\n",
        "                \n",
        "                \n",
        "                \n",
        "                text = \" \".join([idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_text[j][idx]])\n",
        "                predicted_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
        "                actual_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_summary[j][idx]]\n",
        "                \n",
        "                print(\"\\nSample Text\\n\")\n",
        "                print(text)\n",
        "                print(\"\\nSample Predicted Summary\\n\")\n",
        "                for word in predicted_summary:\n",
        "                    if word == '<EOS>':\n",
        "                        break\n",
        "                    else:\n",
        "                        print(word,end=\" \")\n",
        "                print(\"\\n\\nSample Actual Summary\\n\")\n",
        "                for word in actual_summary:\n",
        "                    if word == '<EOS>':\n",
        "                        break\n",
        "                    else:\n",
        "                        print(word,end=\" \")\n",
        "                print(\"\\n\\n\")\n",
        "                \n",
        "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
        "                \n",
        "        total_val_loss=0\n",
        "        total_val_acc=0\n",
        "                \n",
        "        for i in range(0, len(val_batches_text)):\n",
        "            \n",
        "            if i%100==0:\n",
        "                print(\"Validating data # {}\".format(i))\n",
        "\n",
        "            cost, prediction,\\\n",
        "                acc = sess.run([cross_entropy,\n",
        "                                outputs,\n",
        "                                accuracy],\n",
        "                                  feed_dict={tf_text: val_batches_text[i],\n",
        "                                             tf_embd: embd,\n",
        "                                             tf_summary: val_batches_summary[i],\n",
        "                                             tf_true_summary_len: val_batches_true_summary_len[i],\n",
        "                                             tf_train: False})\n",
        "            \n",
        "            total_val_loss += cost\n",
        "            total_val_acc += acc\n",
        "            \n",
        "        avg_val_loss = total_val_loss/len(val_batches_text)\n",
        "        \n",
        "        print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs))\n",
        "        print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text)))\n",
        "        print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text)))\n",
        "        print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss))\n",
        "        print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text)))\n",
        "              \n",
        "        if (avg_val_loss < best_loss):\n",
        "            best_loss = avg_val_loss\n",
        "            save_data={'best_loss':best_loss,'covered_epochs':covered_epochs+epoch+1}\n",
        "            impatience=0\n",
        "            with open('/content/drive/My Drive/Colab Notebooks/summ/release/Model_Backup/Seq2seq_summarization.pkl', 'wb') as fp:\n",
        "                pickle.dump(save_data, fp)\n",
        "            saver.save(sess, save_path='/content/drive/My Drive/Colab Notebooks/summ/release/Model_Backup/Seq2seq_summarization.ckpt')\n",
        "            print(\"\\nModel saved\\n\")\n",
        "              \n",
        "        else:\n",
        "            impatience+=1\n",
        "              \n",
        "        if impatience > patience:\n",
        "              break\n",
        "              \n",
        "              \n",
        "        epoch+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "STARTING TRAINING\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "STARTING VALIDATION\n",
            "\n",
            "\n",
            "Validating data # 0\n",
            "Validating data # 100\n",
            "\n",
            "\n",
            "Epoch: 0\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f3547663d8a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nEpoch: {}\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcovered_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Training Loss: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Training Accuracy: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtotal_train_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Validation Loss: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}